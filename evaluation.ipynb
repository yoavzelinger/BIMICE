{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving MICE for Data Imputation: A Methodological and Practical Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Comparing the performance of the our model to impute missing values.\n",
    "Our model is compared with the following models:\n",
    "1. MICE - Multiple Imputation by Chained Equations (The original model we're trying to improve).\n",
    "2. KNNI - K-Nearest Neighbors Imputation.\n",
    "3. SICE - Single Imputation with Chained Equations.\n",
    "\n",
    "For ablation study, we compared several versions of our improvements:\n",
    "1. Ordered only - MICE where the imputation order is computed using the Bayesian Network structure.\n",
    "2. correlated variables in regression only - MICE where only the correlated variables are used as features to the linear regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 0 - Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Models\n",
    "from reparo import \\\n",
    "    MICE, \\\n",
    "    SICE, \\\n",
    "    KNNImputer\n",
    "from BIMICE import BIMICE\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import root_mean_squared_error as RMSE, \\\n",
    "    mean_squared_error as MSE, \\\n",
    "    mean_absolute_error as MAE\n",
    "\n",
    "metrics_dict = {\n",
    "    \"RMSE\": RMSE,\n",
    "    \"MSE\": MSE,\n",
    "    \"MAE\": MAE\n",
    "}\n",
    "\n",
    "models_dict = {\n",
    "    \"OrdereredOnlyBIMICE\": BIMICE(order_imputations=True, filter_predicators=False),\n",
    "    \"FilteredOnlyBIMICE\": BIMICE(order_imputations=False, filter_predicators=True),\n",
    "    \"BIMICE\": BIMICE(order_imputations=True, filter_predicators=True),\n",
    "    \"MICE\": MICE(),\n",
    "    \"SICE\": SICE(),\n",
    "    \"KNN\": KNNImputer()\n",
    "}\n",
    "\n",
    "baselines_algorithms = [\"MICE\", \"SICE\", \"KNN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1 - Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"data/\"\n",
    "FRAMINGHAM = {\"name\": \"framingham.csv\",\n",
    "              \"numeric_columns\": [\"age\", \"education\", \"cigsPerDay\", \"BPMeds\", \"totChol\", \"sysBP\", \"diaBP\", \"heartRate\", \"glucose\"], # TODO - check about education\n",
    "              }\n",
    "# TODO - add the additional dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose dataset to work with\n",
    "Options are FRAMINGHAM or TODO-CONTINUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FRAMINGHAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_FOLDER + dataset[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4240, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>currentSmoker</th>\n",
       "      <th>cigsPerDay</th>\n",
       "      <th>BPMeds</th>\n",
       "      <th>prevalentStroke</th>\n",
       "      <th>prevalentHyp</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>totChol</th>\n",
       "      <th>sysBP</th>\n",
       "      <th>diaBP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>heartRate</th>\n",
       "      <th>glucose</th>\n",
       "      <th>TenYearCHD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26.97</td>\n",
       "      <td>80.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>28.73</td>\n",
       "      <td>95.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>127.5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>25.34</td>\n",
       "      <td>75.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>28.58</td>\n",
       "      <td>65.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>23.10</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   male  age  education  currentSmoker  cigsPerDay  BPMeds  prevalentStroke  \\\n",
       "0     1   39        4.0              0         0.0     0.0                0   \n",
       "1     0   46        2.0              0         0.0     0.0                0   \n",
       "2     1   48        1.0              1        20.0     0.0                0   \n",
       "3     0   61        3.0              1        30.0     0.0                0   \n",
       "4     0   46        3.0              1        23.0     0.0                0   \n",
       "\n",
       "   prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  heartRate  glucose  \\\n",
       "0             0         0    195.0  106.0   70.0  26.97       80.0     77.0   \n",
       "1             0         0    250.0  121.0   81.0  28.73       95.0     76.0   \n",
       "2             0         0    245.0  127.5   80.0  25.34       75.0     70.0   \n",
       "3             1         0    225.0  150.0   95.0  28.58       65.0    103.0   \n",
       "4             0         0    285.0  130.0   84.0  23.10       85.0     85.0   \n",
       "\n",
       "   TenYearCHD  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           1  \n",
       "4           0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2 - Remove not relevant rows and columns\n",
    "- Remove rows with missing values.\n",
    "- Remove columns with non-numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of missing values: 0\n",
      "New shape: (3671, 9)\n"
     ]
    }
   ],
   "source": [
    "df = df[dataset[\"numeric_columns\"]]\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Verify that there are no missing values\n",
    "print(\"Total amount of missing values:\", df.isnull().sum().sum())\n",
    "print(\"New shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3 - Missing values injection functions\n",
    "We are using two types of missing values injection functions:\n",
    "1. inject_missing_completely_at_random - Injection a portion of missing values in random cells in the dataset.\n",
    "2. inject_missing_per_feature - Injecting a different portion of missing values for each column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: Injecting completely at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_missing_completely_at_random(\n",
    "        df: pd.DataFrame, \n",
    "        missing_rate: float\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Inject missing values completely at random\n",
    "    :param df: DataFrame\n",
    "    :param missing_rate: float\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    mask = np.random.rand(*df.shape) < missing_rate\n",
    "    df[mask] = np.nan\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2: Injecting at random to given columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_missing_per_feature(df: pd.DataFrame,\n",
    "                               missing_rate: float,\n",
    "                               features: list\n",
    " ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Inject missing values per feature\n",
    "    :param df: DataFrame\n",
    "    :param missing_rate: float\n",
    "    :param features: list\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    for feature in features:\n",
    "        mask = np.random.rand(df.shape[0]) < missing_rate\n",
    "        df.loc[mask, feature] = np.nan\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "def inject_missing_values(\n",
    "        data: pd.DataFrame,\n",
    "        columns: list[str],\n",
    "        rows_severity: float # or list[float]\n",
    " ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Inject missing values into the data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : DataFrame\n",
    "        The data to inject missing values into.\n",
    "        \n",
    "    columns : list[str]\n",
    "        The columns to inject missing values into.\n",
    "\n",
    "    rows_severity : float | list[float]\n",
    "        The severity of the missing values to inject. If a float, the same severity is applied to all columns. If a list, the severity is applied to each column in order.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        The data with missing values injected.\n",
    "    \"\"\"\n",
    "    assert isinstance(data, pd.DataFrame), 'data must be a pandas DataFrame'\n",
    "    if isinstance(rows_severity, (int, float)):\n",
    "        rows_severity = [rows_severity] * len(columns)\n",
    "    assert isinstance(rows_severity, list) and all(isinstance(severity, (int, float)) and 0 < severity < 1 for severity in rows_severity), 'rows_severity must be between 0 and 1'\n",
    "    assert isinstance(columns, list) and all(column in data.columns for column in columns), 'not all columns are in the DataFrame'\n",
    "\n",
    "\n",
    "    result_data = data.copy()\n",
    "\n",
    "    for current_column, current_rows_severity in zip(columns, rows_severity):\n",
    "        # Determine the number of missing rows\n",
    "        missing_rows_count = int(len(data) * current_rows_severity)\n",
    "        # Avoid cases where none/all rows are cleared\n",
    "        if missing_rows_count == 0:\n",
    "            missing_rows_count = 1\n",
    "        if missing_rows_count == len(data):\n",
    "            missing_rows_count -= 1\n",
    "        # Inject missing values into the current column\n",
    "        current_missing_rows_indices = result_data.sample(missing_rows_count).index\n",
    "        result_data.loc[current_missing_rows_indices, current_column] = None\n",
    "\n",
    "    return result_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4 - Evaluations\n",
    "We evaluate the results using two tests:\n",
    "1. Over different severities of missing completely at random (ranging from 10% to 40%).\n",
    "2. Over different number of features with missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 1 - Different severities of missing completely at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Severity level: 0.1\n",
      "    Model: OrdereredOnlyBIMICE\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Impute missing values\u001b[39;00m\n\u001b[0;32m     17\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m---> 18\u001b[0m data_with_imputations \u001b[38;5;241m=\u001b[39m \u001b[43mcurrent_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_with_missing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m     21\u001b[0m max_predictors_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\resmet_venv\\lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\resmet_venv\\lib\\site-packages\\reparo\\MICE.py:75\u001b[0m, in \u001b[0;36mMICE.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnp.array\u001b[39m\u001b[38;5;124m'\u001b[39m, y: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnp.array\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m    Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param X: array-like of shape (n_samples, n_features)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m        transformed array\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\resmet_venv\\lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\GitHub\\BIMICE\\BIMICE.py:49\u001b[0m, in \u001b[0;36mBIMICE.transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: np\u001b[38;5;241m.\u001b[39marray, y: np\u001b[38;5;241m.\u001b[39marray \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# Get the relevant predictors for each column\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m     predicators_dict \u001b[38;5;241m=\u001b[39m \u001b[43mBIMICE\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_inverse_dependencies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     imputation_order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(TopologicalSorter(predicators_dict)\u001b[38;5;241m.\u001b[39mstatic_order())\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morder_imputations:\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\GitHub\\BIMICE\\BIMICE.py:40\u001b[0m, in \u001b[0;36mBIMICE._get_inverse_dependencies\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_inverse_dependencies\u001b[39m(X: np\u001b[38;5;241m.\u001b[39marray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    :param X: np.array\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m        input data\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m    :return: dict\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m        inverse dependencies. Each key represents a node and the value is a list of its parents.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     dependencies_edges \u001b[38;5;241m=\u001b[39m \u001b[43mBIMICE\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_bayesian_network_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     inverse_dependencies \u001b[38;5;241m=\u001b[39m {independent: [] \u001b[38;5;28;01mfor\u001b[39;00m independent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])}\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dependent, independent \u001b[38;5;129;01min\u001b[39;00m dependencies_edges:\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\GitHub\\BIMICE\\BIMICE.py:28\u001b[0m, in \u001b[0;36mBIMICE._get_bayesian_network_edges\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_bayesian_network_edges\u001b[39m(X: np\u001b[38;5;241m.\u001b[39marray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m     22\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m    :param X: np.array\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m        input data\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m    :return: list of tuples\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03m        list of edges. Each edge represents conditional dependence between the two nodes (directional).\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m     dependencies \u001b[38;5;241m=\u001b[39m \u001b[43mdf_to_dependencies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     bayesian_network \u001b[38;5;241m=\u001b[39m BayesianNetwork(dependencies)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bayesian_network\u001b[38;5;241m.\u001b[39medges\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\resmet_venv\\lib\\site-packages\\causalnex\\structure\\notears.py:251\u001b[0m, in \u001b[0;36mfrom_pandas\u001b[1;34m(X, max_iter, h_tol, w_threshold, tabu_edges, tabu_parent_nodes, tabu_child_nodes)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tabu_child_nodes:\n\u001b[0;32m    249\u001b[0m     tabu_child_nodes \u001b[38;5;241m=\u001b[39m [col_idx[n] \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m tabu_child_nodes]\n\u001b[1;32m--> 251\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mh_tol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mw_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtabu_edges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtabu_parent_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtabu_child_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m sm \u001b[38;5;241m=\u001b[39m StructureModel()\n\u001b[0;32m    262\u001b[0m sm\u001b[38;5;241m.\u001b[39madd_nodes_from(data\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\resmet_venv\\lib\\site-packages\\causalnex\\structure\\notears.py:121\u001b[0m, in \u001b[0;36mfrom_numpy\u001b[1;34m(X, max_iter, h_tol, w_threshold, tabu_edges, tabu_parent_nodes, tabu_child_nodes)\u001b[0m\n\u001b[0;32m    105\u001b[0m _assert_all_finite(X)\n\u001b[0;32m    107\u001b[0m bnds \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    108\u001b[0m     (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m j\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(d)\n\u001b[0;32m    119\u001b[0m ]\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_learn_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbnds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_tol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_threshold\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\resmet_venv\\lib\\site-packages\\causalnex\\structure\\notears.py:425\u001b[0m, in \u001b[0;36m_learn_structure\u001b[1;34m(X, bnds, max_iter, h_tol, w_threshold)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n_iter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m (rho \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e20\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (h_new \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.25\u001b[39m \u001b[38;5;241m*\u001b[39m h \u001b[38;5;129;01mor\u001b[39;00m h_new \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39minf):\n\u001b[1;32m--> 425\u001b[0m         sol \u001b[38;5;241m=\u001b[39m \u001b[43msopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_est\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    426\u001b[0m         w_new \u001b[38;5;241m=\u001b[39m sol\u001b[38;5;241m.\u001b[39mx\n\u001b[0;32m    427\u001b[0m         h_new \u001b[38;5;241m=\u001b[39m _h(w_new)\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\resmet_venv\\lib\\site-packages\\scipy\\optimize\\_minimize.py:713\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    710\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    711\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 713\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m    714\u001b[0m                            callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    716\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    717\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\resmet_venv\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:407\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    401\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[0;32m    410\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\resmet_venv\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:296\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\resmet_venv\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:262\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\resmet_venv\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:163\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\resmet_venv\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:145\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\resmet_venv\\lib\\site-packages\\causalnex\\structure\\notears.py:389\u001b[0m, in \u001b[0;36m_learn_structure.<locals>._func\u001b[1;34m(w)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;124;03mObjective function that the NOTEARS algorithm tries to minimise.\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;124;03m    float: objective.\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    388\u001b[0m W \u001b[38;5;241m=\u001b[39m w\u001b[38;5;241m.\u001b[39mreshape([d, d])\n\u001b[1;32m--> 389\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m/\u001b[39m n \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msquare(np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meye\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfro\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    390\u001b[0m h \u001b[38;5;241m=\u001b[39m _h(W)\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m rho \u001b[38;5;241m*\u001b[39m h \u001b[38;5;241m*\u001b[39m h \u001b[38;5;241m+\u001b[39m alpha \u001b[38;5;241m*\u001b[39m h\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "original_data = df.to_numpy()\n",
    "# Severity levels\n",
    "severity_levels = [0.1, 0.2, 0.3, 0.4]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for current_severity_level in severity_levels:\n",
    "    print(\"Severity level:\", current_severity_level)\n",
    "    current_severity_results = {}\n",
    "    # Inject missing values\n",
    "    data_with_missing = inject_missing_completely_at_random(df, current_severity_level).to_numpy()\n",
    "    missing_features_count = np.sum(np.isnan(data_with_missing))\n",
    "\n",
    "    for current_model_name, current_model in models_dict.items():\n",
    "        print(\"    Model:\", current_model_name)\n",
    "        # Impute missing values\n",
    "        start_time = datetime.now()\n",
    "        data_with_imputations = current_model.fit_transform(data_with_missing)\n",
    "        end_time = datetime.now()\n",
    "        \n",
    "        max_predictors_count = len(df.columns) - 1\n",
    "        predicators_counts = [max_predictors_count] * len(df.columns)\n",
    "        if current_model_name not in baselines_algorithms:\n",
    "            data_with_imputations, predicators_counts = data_with_imputations\n",
    "        \n",
    "        # Calculate metrics\n",
    "        current_severity_results[current_model_name] = {metric_name: metric_func(original_data, data_with_imputations) \n",
    "                                                        for metric_name, metric_func in metrics_dict.items()}\n",
    "        # Calculate MAPE\n",
    "        features_mape_sum = 0\n",
    "        for column_index, predicators_count in enumerate(predicators_counts):\n",
    "            features_mape_sum += metrics_dict[\"MAE\"](original_data[:, column_index], data_with_imputations[:, column_index]) / max(max_predictors_count - predicators_count, 1)\n",
    "        current_severity_results[current_model_name][\"time\"] = (end_time - start_time).total_seconds() * 1000\n",
    "        current_severity_results[current_model_name][\"MAPE\"] = features_mape_sum / missing_features_count\n",
    "    print(current_severity_results)\n",
    "    results[current_severity_level] = current_severity_results\n",
    "    with open(\"MCAR_results.pkl\", \"wb\") as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 2 - Different number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of columns: 1, Random run: 1\n",
      "    Model: OrdereredOnlyBIMICE\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Impute missing values\u001b[39;00m\n\u001b[0;32m     20\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m---> 21\u001b[0m data_with_imputations \u001b[38;5;241m=\u001b[39m \u001b[43mcurrent_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_with_missing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m     24\u001b[0m max_predictors_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\resmet_venv\\lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\resmet_venv\\lib\\site-packages\\reparo\\MICE.py:75\u001b[0m, in \u001b[0;36mMICE.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnp.array\u001b[39m\u001b[38;5;124m'\u001b[39m, y: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnp.array\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m    Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param X: array-like of shape (n_samples, n_features)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m        transformed array\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\resmet_venv\\lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\GitHub\\BIMICE\\BIMICE.py:49\u001b[0m, in \u001b[0;36mBIMICE.transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: np\u001b[38;5;241m.\u001b[39marray, y: np\u001b[38;5;241m.\u001b[39marray \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# Get the relevant predictors for each column\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m     predicators_dict \u001b[38;5;241m=\u001b[39m \u001b[43mBIMICE\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_inverse_dependencies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     imputation_order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(TopologicalSorter(predicators_dict)\u001b[38;5;241m.\u001b[39mstatic_order())\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morder_imputations:\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\GitHub\\BIMICE\\BIMICE.py:40\u001b[0m, in \u001b[0;36mBIMICE._get_inverse_dependencies\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_inverse_dependencies\u001b[39m(X: np\u001b[38;5;241m.\u001b[39marray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    :param X: np.array\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m        input data\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m    :return: dict\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m        inverse dependencies. Each key represents a node and the value is a list of its parents.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     dependencies_edges \u001b[38;5;241m=\u001b[39m \u001b[43mBIMICE\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_bayesian_network_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     inverse_dependencies \u001b[38;5;241m=\u001b[39m {independent: [] \u001b[38;5;28;01mfor\u001b[39;00m independent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])}\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dependent, independent \u001b[38;5;129;01min\u001b[39;00m dependencies_edges:\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\GitHub\\BIMICE\\BIMICE.py:28\u001b[0m, in \u001b[0;36mBIMICE._get_bayesian_network_edges\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_bayesian_network_edges\u001b[39m(X: np\u001b[38;5;241m.\u001b[39marray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m     22\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m    :param X: np.array\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m        input data\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m    :return: list of tuples\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03m        list of edges. Each edge represents conditional dependence between the two nodes (directional).\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m     dependencies \u001b[38;5;241m=\u001b[39m \u001b[43mdf_to_dependencies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     bayesian_network \u001b[38;5;241m=\u001b[39m BayesianNetwork(dependencies)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bayesian_network\u001b[38;5;241m.\u001b[39medges\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\resmet_venv\\lib\\site-packages\\causalnex\\structure\\notears.py:251\u001b[0m, in \u001b[0;36mfrom_pandas\u001b[1;34m(X, max_iter, h_tol, w_threshold, tabu_edges, tabu_parent_nodes, tabu_child_nodes)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tabu_child_nodes:\n\u001b[0;32m    249\u001b[0m     tabu_child_nodes \u001b[38;5;241m=\u001b[39m [col_idx[n] \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m tabu_child_nodes]\n\u001b[1;32m--> 251\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mh_tol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mw_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtabu_edges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtabu_parent_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtabu_child_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m sm \u001b[38;5;241m=\u001b[39m StructureModel()\n\u001b[0;32m    262\u001b[0m sm\u001b[38;5;241m.\u001b[39madd_nodes_from(data\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\resmet_venv\\lib\\site-packages\\causalnex\\structure\\notears.py:121\u001b[0m, in \u001b[0;36mfrom_numpy\u001b[1;34m(X, max_iter, h_tol, w_threshold, tabu_edges, tabu_parent_nodes, tabu_child_nodes)\u001b[0m\n\u001b[0;32m    105\u001b[0m _assert_all_finite(X)\n\u001b[0;32m    107\u001b[0m bnds \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    108\u001b[0m     (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m j\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(d)\n\u001b[0;32m    119\u001b[0m ]\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_learn_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbnds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_tol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_threshold\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\resmet_venv\\lib\\site-packages\\causalnex\\structure\\notears.py:425\u001b[0m, in \u001b[0;36m_learn_structure\u001b[1;34m(X, bnds, max_iter, h_tol, w_threshold)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n_iter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m (rho \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e20\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (h_new \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.25\u001b[39m \u001b[38;5;241m*\u001b[39m h \u001b[38;5;129;01mor\u001b[39;00m h_new \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39minf):\n\u001b[1;32m--> 425\u001b[0m         sol \u001b[38;5;241m=\u001b[39m \u001b[43msopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_est\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    426\u001b[0m         w_new \u001b[38;5;241m=\u001b[39m sol\u001b[38;5;241m.\u001b[39mx\n\u001b[0;32m    427\u001b[0m         h_new \u001b[38;5;241m=\u001b[39m _h(w_new)\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\resmet_venv\\lib\\site-packages\\scipy\\optimize\\_minimize.py:713\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    710\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    711\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 713\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m    714\u001b[0m                            callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    716\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    717\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\resmet_venv\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:407\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    401\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[0;32m    410\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\resmet_venv\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:297\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[1;32m--> 297\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\resmet_venv\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:267\u001b[0m, in \u001b[0;36mScalarFunction._update_grad\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_update_grad\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated:\n\u001b[1;32m--> 267\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_grad_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\resmet_venv\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:175\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_grad\u001b[1;34m()\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mupdate_grad\u001b[39m():\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\resmet_venv\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:172\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.grad_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgrad_wrapped\u001b[39m(x):\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39matleast_1d(\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\resmet_venv\\lib\\site-packages\\causalnex\\structure\\notears.py:405\u001b[0m, in \u001b[0;36m_learn_structure.<locals>._grad\u001b[1;34m(w)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;124;03mGradient function used to compute next step in NOTEARS algorithm.\u001b[39;00m\n\u001b[0;32m    396\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m    np.ndarray: gradient vector.\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    404\u001b[0m W \u001b[38;5;241m=\u001b[39m w\u001b[38;5;241m.\u001b[39mreshape([d, d])\n\u001b[1;32m--> 405\u001b[0m loss_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m n \u001b[38;5;241m*\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdot(np\u001b[38;5;241m.\u001b[39meye(d, d) \u001b[38;5;241m-\u001b[39m W)\n\u001b[0;32m    406\u001b[0m E \u001b[38;5;241m=\u001b[39m slin\u001b[38;5;241m.\u001b[39mexpm(W \u001b[38;5;241m*\u001b[39m W)\n\u001b[0;32m    407\u001b[0m obj_grad \u001b[38;5;241m=\u001b[39m loss_grad \u001b[38;5;241m+\u001b[39m (rho \u001b[38;5;241m*\u001b[39m (np\u001b[38;5;241m.\u001b[39mtrace(E) \u001b[38;5;241m-\u001b[39m d) \u001b[38;5;241m+\u001b[39m alpha) \u001b[38;5;241m*\u001b[39m E\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m*\u001b[39m W \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "original_data = df.to_numpy()\n",
    "random_runs = 10\n",
    "# Severity levels\n",
    "column_severity_range = (0.05, 0.4)\n",
    "missing_features_counts = [1, 3, 5, 7, 9]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for missing_features_count in missing_features_counts:\n",
    "    current_results = {}\n",
    "    print(\"Amount of columns:\", missing_features_count)\n",
    "    for random_run in range(random_runs):\n",
    "        print(\"    Random run:\", random_run)\n",
    "        # Draw columns and severity level\n",
    "        columns_to_inject_missing = np.random.choice(df.columns, missing_features_count, replace=False)\n",
    "        severity_level = np.random.uniform(*column_severity_range)\n",
    "        data_with_missing = inject_missing_per_feature(df, severity_level, columns_to_inject_missing).to_numpy()\n",
    "        for current_model_name, current_model in models_dict.items():\n",
    "            print(\"        Model:\", current_model_name)\n",
    "            # Impute missing values\n",
    "            start_time = datetime.now()\n",
    "            data_with_imputations = current_model.fit_transform(data_with_missing)\n",
    "            end_time = datetime.now()\n",
    "            \n",
    "            max_predictors_count = len(df.columns) - 1\n",
    "            predicators_counts = [max_predictors_count] * len(df.columns)\n",
    "            if current_model_name not in baselines_algorithms:\n",
    "                data_with_imputations, predicators_counts = data_with_imputations\n",
    "            \n",
    "            # Calculate metrics\n",
    "            current_results[current_model_name] = {metric_name: 0 for metric_name in (list(metrics_dict.keys()) + [\"time\", \"MAPE\"])}\n",
    "            for metric_name, metric_func in metrics_dict.items():\n",
    "                current_results[current_model_name][metric_name] += (metric_func(original_data, data_with_imputations) / random_runs) # Average on the fly\n",
    "            # Calculate MAPE\n",
    "            features_mape_sum = 0\n",
    "            for column_index, predicators_count in enumerate(predicators_counts):\n",
    "                features_mape_sum += metrics_dict[\"MAE\"](original_data[:, column_index], data_with_imputations[:, column_index]) / max(max_predictors_count - predicators_count, 1)\n",
    "            current_results[current_model_name][\"time\"] = ((end_time - start_time).total_seconds() * 1000) / random_runs\n",
    "            current_results[current_model_name][\"MAPE\"] = (features_mape_sum / missing_features_count) / random_runs\n",
    "    print(current_results)\n",
    "    results[missing_features_count] = current_results\n",
    "    with open(\"MCAR_results.pkl\", \"wb\") as f:\n",
    "        pickle.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resmet_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
